\documentclass{article}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\begin{document}
\begin{titlepage}
   \begin{center}
       \vspace*{1cm}

       \huge \textbf{Mathematics for Machine Learning}

       \vspace{0.5cm}
        
            
       \vspace{1.5cm}

      
      \large \textbf{Aditya Reddy, Claire Guo, and Cole Wasserman}

      \vspace{35 mm}
            
       Final Project for Discrete Mathematics\\
            
       \vspace{0.8cm}
     
       \includegraphics[width=0.4\textwidth]{university}
        \vspace{20 mm}     
       
       New York University\\
       Professor Normand\\
       December 2020\
            
   \end{center}
\end{titlepage}

\pagestyle{fancy}
\fancyhf{}
%\rhead{Aditya Reddy, Claire Guo, and Cole Wasserman}
\lhead{Mathemaics for Machine Learning}
\rfoot{Page \thepage}
\tableofcontents
\newpage

\section{Introduction}

Machine learning is the process of mimicking human-like intuition through the capabilities of computer algorithms and the ability for machines to improve themselves through experience. Its applications are demonstrated through things such as data modeling, image/speech recognition, web recommendations/advertisements, etc. Unlike humans, who are able to see a given problem and instinctively produce a solution or method of solving the problem using our previous knowledge of what has worked and what has not, machines don’t have this capability; instead, they are only able to do exactly as dictated in the algorithms that we provide them with. However, the benefit of this is that machines are significantly faster at performing repeated tasks than humans are. Take the problem of finding the sum of $1+2+3+\dots+100$ for example. While there is obviously a formula to help us solve this problem faster, even applying it may take us a minute or so. A machine, however, given the correct code, would be able to find the solution in a fraction of a second, even by the method of iterating through each integer. Although that application may seem simple, the capacities of machines expand way beyond just solving addition problems. Machine learning takes the computer’s ability to perform repetitive tasks quickly and translates it into a means to find patterns. The types of learning can be broken down into three categories: supervised, unsupervised, and reinforcement. Supervised learning involves providing the machine with the exact patterns to look for, unsupervised learning involves the machine finding whatever patterns there are, and reinforcement learning involves having a clear objective and trying out different things until the objective is met. These learning methods all involve elements of math (multivariable calculus and linear algebra, most frequently), and in this report we will explore their applications in two fundamental concepts of machine learning: cost functions and gradient descent. 

\section{Main Section}
\textbf{Linear Regression}\newline
The most basic and standard concept involved in the understanding of machine learning algorithms, linear regression is the way of modeling a group of data through a linear representation, or in other words: $y=ax+b$. The process involves finding a linear model such that the sum of the differences between the actual values that are greater than their predicted values and the predicted values is roughly equal to the sum of the differences between the predicted values and the actual values that are less than their predicted values.\smallskip\newline
Example with pictures on a random set of points:\bigskip\newline
\textbf{Cost Function}\newline
Used to maximize accuracy, this describes how closely a linear regression model fits a set of data, or in other words, the error between the actual and predicted values. The higher the value (or cost), the greater the error (or cost); the lesser the value, the lesser the error. It is represented by the function: $a$\smallskip\newline
Code Implementation (Python): \bigskip\newline
\textbf{Gradient Descent}\newline
This optimization algorithm finds the best model to minimize error.\smallskip\newline
Code Implementation (Python): \newline
\section{Proofs}
\begin{enumerate}
    \item \textbf{Proof of the Cost Function for Univariate Linear Regression}\\
    \medskip Prove that the cost function be defined by: \[
    J(\theta_0,\theta_1) = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
    \]
    \textbf{Definitions}\\
    For all $m$ $\in \mathbb{N}$, let $A$ be an arbitrary function where \smallskip $A = \{ (x_1, y_1), (x_2, y_2) \dots (x_m, y_m) \}$.\\
    \smallskip Note that set $A$ represents the number of points in the given data set.\\
    \smallskip Let $h_{\theta}$ be the linear function that we are trying to compute the cost of.\\
    \smallskip $h_{\theta}(x)$ = $\theta_0 + \theta_1x$ where $\theta_0$ is the y-intercept and $\theta_1$ is the slope.

    
     \begin{proof}
     \begin{align*}
          J(\theta_0,\theta_1)&=\frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}\\
          &=\frac{1}{m}[(h_\theta (x_1)-y_1)^2+(h_\theta (x_2)-y_2)^2+...+(h_\theta (x_m)-y_m)^2]
     \end{align*}
     \end{proof}
    \item \textbf{Proof of gradient descent with fixed step size}
    \[
    \theta_{j} := \theta_{j} - \alpha \frac{\partial}{\partial \theta_{j}}   J(\theta)
    \]
\end{enumerate}
\section{Conclusion}
Here, we will write a conclusion about the project.
\section{References}
Here, we will list all of the references we use in our project.

\end{document}